{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f28d4e4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T07:39:13.280538Z",
     "start_time": "2023-04-12T07:39:02.983897Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-30 02:16:19.432633: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-30 02:16:19.464827: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-30 02:16:19.464857: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-30 02:16:19.465668: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-30 02:16:19.471040: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-30 02:16:20.160945: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.layers import Input, Conv1D, Activation, BatchNormalization\n",
    "from tensorflow.keras.layers import MaxPool1D, Dropout, GlobalAveragePooling1D, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdf71524",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T07:40:33.381845Z",
     "start_time": "2023-04-12T07:40:33.378607Z"
    }
   },
   "outputs": [],
   "source": [
    "wave_type = ['gamma', 'beta', 'alpha', 'theta', 'delta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed83084b-e6d2-436e-9268-bdf33cece267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.5</td>\n",
       "      <td>88.6</td>\n",
       "      <td>88.6</td>\n",
       "      <td>88.6</td>\n",
       "      <td>88.6</td>\n",
       "      <td>88.5</td>\n",
       "      <td>88.4</td>\n",
       "      <td>88.3</td>\n",
       "      <td>88.1</td>\n",
       "      <td>88.0</td>\n",
       "      <td>...</td>\n",
       "      <td>83.5</td>\n",
       "      <td>83.6</td>\n",
       "      <td>83.5</td>\n",
       "      <td>83.3</td>\n",
       "      <td>83.2</td>\n",
       "      <td>83.1</td>\n",
       "      <td>83.0</td>\n",
       "      <td>82.9</td>\n",
       "      <td>82.8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.3</td>\n",
       "      <td>87.2</td>\n",
       "      <td>87.2</td>\n",
       "      <td>87.1</td>\n",
       "      <td>87.1</td>\n",
       "      <td>87.0</td>\n",
       "      <td>86.9</td>\n",
       "      <td>86.8</td>\n",
       "      <td>86.8</td>\n",
       "      <td>86.7</td>\n",
       "      <td>...</td>\n",
       "      <td>83.8</td>\n",
       "      <td>83.9</td>\n",
       "      <td>84.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>83.9</td>\n",
       "      <td>83.7</td>\n",
       "      <td>83.6</td>\n",
       "      <td>83.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>86.1</td>\n",
       "      <td>85.9</td>\n",
       "      <td>85.7</td>\n",
       "      <td>85.5</td>\n",
       "      <td>85.4</td>\n",
       "      <td>85.3</td>\n",
       "      <td>85.3</td>\n",
       "      <td>85.3</td>\n",
       "      <td>85.3</td>\n",
       "      <td>85.2</td>\n",
       "      <td>...</td>\n",
       "      <td>82.5</td>\n",
       "      <td>82.5</td>\n",
       "      <td>82.6</td>\n",
       "      <td>82.8</td>\n",
       "      <td>83.0</td>\n",
       "      <td>83.1</td>\n",
       "      <td>83.2</td>\n",
       "      <td>83.4</td>\n",
       "      <td>83.6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>85.4</td>\n",
       "      <td>85.3</td>\n",
       "      <td>85.2</td>\n",
       "      <td>85.2</td>\n",
       "      <td>85.1</td>\n",
       "      <td>85.0</td>\n",
       "      <td>85.1</td>\n",
       "      <td>85.1</td>\n",
       "      <td>85.1</td>\n",
       "      <td>85.0</td>\n",
       "      <td>...</td>\n",
       "      <td>83.8</td>\n",
       "      <td>83.7</td>\n",
       "      <td>83.6</td>\n",
       "      <td>83.6</td>\n",
       "      <td>83.8</td>\n",
       "      <td>84.0</td>\n",
       "      <td>84.2</td>\n",
       "      <td>84.5</td>\n",
       "      <td>84.6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85.2</td>\n",
       "      <td>85.2</td>\n",
       "      <td>85.2</td>\n",
       "      <td>85.1</td>\n",
       "      <td>85.1</td>\n",
       "      <td>85.1</td>\n",
       "      <td>85.2</td>\n",
       "      <td>85.3</td>\n",
       "      <td>85.3</td>\n",
       "      <td>85.4</td>\n",
       "      <td>...</td>\n",
       "      <td>84.6</td>\n",
       "      <td>84.7</td>\n",
       "      <td>84.6</td>\n",
       "      <td>84.5</td>\n",
       "      <td>84.5</td>\n",
       "      <td>84.5</td>\n",
       "      <td>84.6</td>\n",
       "      <td>84.5</td>\n",
       "      <td>84.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>91.3</td>\n",
       "      <td>91.4</td>\n",
       "      <td>91.4</td>\n",
       "      <td>91.4</td>\n",
       "      <td>91.3</td>\n",
       "      <td>91.2</td>\n",
       "      <td>91.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>91.0</td>\n",
       "      <td>91.1</td>\n",
       "      <td>...</td>\n",
       "      <td>93.9</td>\n",
       "      <td>93.8</td>\n",
       "      <td>93.7</td>\n",
       "      <td>93.6</td>\n",
       "      <td>93.5</td>\n",
       "      <td>93.4</td>\n",
       "      <td>93.5</td>\n",
       "      <td>93.4</td>\n",
       "      <td>93.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>92.8</td>\n",
       "      <td>93.0</td>\n",
       "      <td>93.1</td>\n",
       "      <td>93.1</td>\n",
       "      <td>93.0</td>\n",
       "      <td>92.8</td>\n",
       "      <td>92.8</td>\n",
       "      <td>92.8</td>\n",
       "      <td>92.8</td>\n",
       "      <td>93.0</td>\n",
       "      <td>...</td>\n",
       "      <td>93.5</td>\n",
       "      <td>93.4</td>\n",
       "      <td>93.3</td>\n",
       "      <td>93.1</td>\n",
       "      <td>92.8</td>\n",
       "      <td>92.6</td>\n",
       "      <td>92.4</td>\n",
       "      <td>92.1</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>93.7</td>\n",
       "      <td>93.8</td>\n",
       "      <td>93.8</td>\n",
       "      <td>93.9</td>\n",
       "      <td>94.0</td>\n",
       "      <td>94.1</td>\n",
       "      <td>94.2</td>\n",
       "      <td>94.2</td>\n",
       "      <td>94.1</td>\n",
       "      <td>93.9</td>\n",
       "      <td>...</td>\n",
       "      <td>93.1</td>\n",
       "      <td>93.4</td>\n",
       "      <td>93.6</td>\n",
       "      <td>93.8</td>\n",
       "      <td>93.7</td>\n",
       "      <td>93.6</td>\n",
       "      <td>93.4</td>\n",
       "      <td>93.2</td>\n",
       "      <td>93.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>92.5</td>\n",
       "      <td>92.2</td>\n",
       "      <td>92.0</td>\n",
       "      <td>91.8</td>\n",
       "      <td>91.6</td>\n",
       "      <td>91.5</td>\n",
       "      <td>91.3</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.9</td>\n",
       "      <td>90.7</td>\n",
       "      <td>...</td>\n",
       "      <td>91.8</td>\n",
       "      <td>91.4</td>\n",
       "      <td>91.0</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.0</td>\n",
       "      <td>89.7</td>\n",
       "      <td>89.4</td>\n",
       "      <td>89.1</td>\n",
       "      <td>88.8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>89.1</td>\n",
       "      <td>89.1</td>\n",
       "      <td>89.1</td>\n",
       "      <td>89.2</td>\n",
       "      <td>89.2</td>\n",
       "      <td>89.2</td>\n",
       "      <td>89.2</td>\n",
       "      <td>89.2</td>\n",
       "      <td>89.2</td>\n",
       "      <td>89.2</td>\n",
       "      <td>...</td>\n",
       "      <td>87.1</td>\n",
       "      <td>87.0</td>\n",
       "      <td>86.8</td>\n",
       "      <td>86.7</td>\n",
       "      <td>86.6</td>\n",
       "      <td>86.7</td>\n",
       "      <td>86.8</td>\n",
       "      <td>86.9</td>\n",
       "      <td>86.9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>169 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2     3     4     5     6     7     8     9  ...     7  \\\n",
       "0    88.5  88.6  88.6  88.6  88.6  88.5  88.4  88.3  88.1  88.0  ...  83.5   \n",
       "1    87.3  87.2  87.2  87.1  87.1  87.0  86.9  86.8  86.8  86.7  ...  83.8   \n",
       "2    86.1  85.9  85.7  85.5  85.4  85.3  85.3  85.3  85.3  85.2  ...  82.5   \n",
       "3    85.4  85.3  85.2  85.2  85.1  85.0  85.1  85.1  85.1  85.0  ...  83.8   \n",
       "4    85.2  85.2  85.2  85.1  85.1  85.1  85.2  85.3  85.3  85.4  ...  84.6   \n",
       "..    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "164  91.3  91.4  91.4  91.4  91.3  91.2  91.2  91.1  91.0  91.1  ...  93.9   \n",
       "165  92.8  93.0  93.1  93.1  93.0  92.8  92.8  92.8  92.8  93.0  ...  93.5   \n",
       "166  93.7  93.8  93.8  93.9  94.0  94.1  94.2  94.2  94.1  93.9  ...  93.1   \n",
       "167  92.5  92.2  92.0  91.8  91.6  91.5  91.3  91.1  90.9  90.7  ...  91.8   \n",
       "168  89.1  89.1  89.1  89.2  89.2  89.2  89.2  89.2  89.2  89.2  ...  87.1   \n",
       "\n",
       "        8     9    10    11    12    13    14    15  label  \n",
       "0    83.6  83.5  83.3  83.2  83.1  83.0  82.9  82.8    0.0  \n",
       "1    83.9  84.0  84.0  84.0  83.9  83.7  83.6  83.5    0.0  \n",
       "2    82.5  82.6  82.8  83.0  83.1  83.2  83.4  83.6    0.0  \n",
       "3    83.7  83.6  83.6  83.8  84.0  84.2  84.5  84.6    0.0  \n",
       "4    84.7  84.6  84.5  84.5  84.5  84.6  84.5  84.4    0.0  \n",
       "..    ...   ...   ...   ...   ...   ...   ...   ...    ...  \n",
       "164  93.8  93.7  93.6  93.5  93.4  93.5  93.4  93.4    0.0  \n",
       "165  93.4  93.3  93.1  92.8  92.6  92.4  92.1  92.0    0.0  \n",
       "166  93.4  93.6  93.8  93.7  93.6  93.4  93.2  93.1    0.0  \n",
       "167  91.4  91.0  90.4  90.0  89.7  89.4  89.1  88.8    0.0  \n",
       "168  87.0  86.8  86.7  86.6  86.7  86.8  86.9  86.9    0.0  \n",
       "\n",
       "[169 rows x 81 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "for wt in wave_type:\n",
    "    df2 = pd.read_csv(wt + '.csv')\n",
    "    df = pd.concat([df, df2.drop('label', axis=1)], axis=1)\n",
    "\n",
    "df['label'] = df2['label']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "876a0930-ce5a-4012-9312-2980c64839ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop('label', axis=1), df['label']\n",
    "y = y.astype(int)\n",
    "M, m = X.max(), X.min()\n",
    "X = (X - m) / (M - m)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ab77656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch conv1d default stride=1, padding=0\n",
    "# keras conv1d default stride=1, padding=\"valid\"\n",
    "\n",
    "def conv_block(output, k, s, p):\n",
    "    return tf.keras.Sequential([\n",
    "                Conv1D(output, kernel_size=k, strides=s, padding=p), \n",
    "                Activation('gelu'),\n",
    "                BatchNormalization(),\n",
    "                Conv1D(output, kernel_size=1), \n",
    "                Activation('gelu'),\n",
    "                BatchNormalization(),\n",
    "                Conv1D(output, kernel_size=1), \n",
    "                Activation('gelu'),\n",
    "                BatchNormalization()\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02924daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(80,1)),\n",
    "        conv_block(4, 2, 2, 'valid'),\n",
    "        MaxPool1D(pool_size=3, strides=2, padding='same'),\n",
    "        conv_block(16, 2, 1, 'same'),\n",
    "        MaxPool1D(pool_size=3, strides=2, padding='same'),\n",
    "        conv_block(64, 2, 1, 'same'),\n",
    "        MaxPool1D(pool_size=3, strides=2, padding='same'),\n",
    "        Dropout(0.4),\n",
    "        conv_block(128, 2, 1, 'same'),\n",
    "        GlobalAveragePooling1D(),\n",
    "        Dense(1, 'sigmoid')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c36bfe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40c5b5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (None, 40, 4)             100       \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 20, 4)             0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   (None, 20, 16)            880       \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPoolin  (None, 10, 16)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " sequential_2 (Sequential)   (None, 10, 64)            11200     \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPoolin  (None, 5, 64)             0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 5, 64)             0         \n",
      "                                                                 \n",
      " sequential_3 (Sequential)   (None, 5, 128)            51072     \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 128)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 63381 (247.58 KB)\n",
      "Trainable params: 62109 (242.61 KB)\n",
      "Non-trainable params: 1272 (4.97 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.build(input_shape=(None, 80, 1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94c15128",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T07:45:22.802037Z",
     "start_time": "2023-04-12T07:45:22.797720Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, epoch, X, y, X_val, y_val):\n",
    "    es = EarlyStopping(monitor='val_accuracy', patience=20)\n",
    "    mc = ModelCheckpoint('EEG_single_model_best.h5', monitor='val_accuracy', verbose=0)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(x=X, y=y, batch_size=8, epochs=epoch, validation_data=(X_val, y_val), callbacks=[es, mc])\n",
    "    model.save('EEG_single_model_last.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bce84ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T07:49:40.564541Z",
     "start_time": "2023-04-12T07:49:30.539130Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17/17 [==============================] - 5s 55ms/step - loss: 0.5563 - accuracy: 0.7037 - val_loss: 0.6763 - val_accuracy: 0.6176\n",
      "Epoch 2/100\n",
      " 5/17 [=======>......................] - ETA: 0s - loss: 0.5790 - accuracy: 0.7250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 32ms/step - loss: 0.4652 - accuracy: 0.7704 - val_loss: 0.6657 - val_accuracy: 0.6176\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.2849 - accuracy: 0.8889 - val_loss: 0.6695 - val_accuracy: 0.6176\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 1s 33ms/step - loss: 0.2361 - accuracy: 0.9333 - val_loss: 0.6680 - val_accuracy: 0.6176\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.1453 - accuracy: 0.9481 - val_loss: 0.6663 - val_accuracy: 0.6176\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 0s 30ms/step - loss: 0.1458 - accuracy: 0.9481 - val_loss: 0.6635 - val_accuracy: 0.6176\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.1634 - accuracy: 0.9333 - val_loss: 0.6644 - val_accuracy: 0.6176\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.1657 - accuracy: 0.9333 - val_loss: 0.6671 - val_accuracy: 0.6176\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.1576 - accuracy: 0.9407 - val_loss: 0.6730 - val_accuracy: 0.6176\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 0s 30ms/step - loss: 0.2287 - accuracy: 0.9259 - val_loss: 0.7185 - val_accuracy: 0.3824\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 0s 30ms/step - loss: 0.1349 - accuracy: 0.9630 - val_loss: 0.6826 - val_accuracy: 0.7941\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - 0s 30ms/step - loss: 0.0976 - accuracy: 0.9630 - val_loss: 1.1142 - val_accuracy: 0.3824\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - 0s 30ms/step - loss: 0.1173 - accuracy: 0.9333 - val_loss: 1.0183 - val_accuracy: 0.3824\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - 0s 30ms/step - loss: 0.0576 - accuracy: 0.9852 - val_loss: 1.2412 - val_accuracy: 0.3824\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - 0s 30ms/step - loss: 0.0404 - accuracy: 0.9926 - val_loss: 1.1562 - val_accuracy: 0.3824\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.0649 - accuracy: 0.9852 - val_loss: 2.0356 - val_accuracy: 0.3824\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - 0s 30ms/step - loss: 0.1254 - accuracy: 0.9556 - val_loss: 1.7071 - val_accuracy: 0.3824\n",
      "Epoch 18/100\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.1763 - accuracy: 0.9111 - val_loss: 1.7424 - val_accuracy: 0.3824\n",
      "Epoch 19/100\n",
      "17/17 [==============================] - 0s 30ms/step - loss: 0.1110 - accuracy: 0.9556 - val_loss: 1.8555 - val_accuracy: 0.3824\n",
      "Epoch 20/100\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.0633 - accuracy: 0.9630 - val_loss: 1.6265 - val_accuracy: 0.3824\n",
      "Epoch 21/100\n",
      "17/17 [==============================] - 1s 32ms/step - loss: 0.1675 - accuracy: 0.9481 - val_loss: 1.2599 - val_accuracy: 0.3824\n",
      "Epoch 22/100\n",
      "17/17 [==============================] - 1s 30ms/step - loss: 0.1111 - accuracy: 0.9630 - val_loss: 0.9963 - val_accuracy: 0.4706\n",
      "Epoch 23/100\n",
      "17/17 [==============================] - 1s 32ms/step - loss: 0.0799 - accuracy: 0.9556 - val_loss: 0.3159 - val_accuracy: 0.9118\n",
      "Epoch 24/100\n",
      "17/17 [==============================] - 1s 32ms/step - loss: 0.0906 - accuracy: 0.9630 - val_loss: 0.1707 - val_accuracy: 0.9706\n",
      "Epoch 25/100\n",
      "17/17 [==============================] - 0s 30ms/step - loss: 0.0871 - accuracy: 0.9630 - val_loss: 0.1688 - val_accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "17/17 [==============================] - 0s 29ms/step - loss: 0.0622 - accuracy: 0.9778 - val_loss: 0.1380 - val_accuracy: 0.9118\n",
      "Epoch 27/100\n",
      "17/17 [==============================] - 0s 30ms/step - loss: 0.2385 - accuracy: 0.8963 - val_loss: 0.3230 - val_accuracy: 0.8529\n",
      "Epoch 28/100\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.0533 - accuracy: 0.9778 - val_loss: 0.1052 - val_accuracy: 0.9706\n",
      "Epoch 29/100\n",
      "17/17 [==============================] - 1s 32ms/step - loss: 0.0977 - accuracy: 0.9778 - val_loss: 0.0260 - val_accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "17/17 [==============================] - 0s 30ms/step - loss: 0.0994 - accuracy: 0.9704 - val_loss: 0.0198 - val_accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "17/17 [==============================] - 0s 29ms/step - loss: 0.0323 - accuracy: 0.9852 - val_loss: 0.0165 - val_accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.1033 - accuracy: 0.9630 - val_loss: 0.0141 - val_accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.0336 - accuracy: 0.9926 - val_loss: 0.0109 - val_accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "17/17 [==============================] - 1s 30ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.0098 - val_accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.0406 - accuracy: 0.9926 - val_loss: 0.0044 - val_accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.0490 - accuracy: 0.9704 - val_loss: 0.0074 - val_accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "17/17 [==============================] - 0s 29ms/step - loss: 0.0429 - accuracy: 0.9778 - val_loss: 0.0065 - val_accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "17/17 [==============================] - 0s 30ms/step - loss: 0.0461 - accuracy: 0.9926 - val_loss: 0.0054 - val_accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.0269 - accuracy: 0.9926 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "17/17 [==============================] - 1s 30ms/step - loss: 0.0581 - accuracy: 0.9778 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.0479 - accuracy: 0.9704 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "17/17 [==============================] - 0s 30ms/step - loss: 0.2247 - accuracy: 0.9407 - val_loss: 0.0153 - val_accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "17/17 [==============================] - 1s 30ms/step - loss: 0.0336 - accuracy: 0.9926 - val_loss: 0.0115 - val_accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.0213 - accuracy: 0.9926 - val_loss: 0.0054 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# X_train, X_valid, y_train, y_valid = preprocessing('gamma')\n",
    "model = get_model()\n",
    "train(model, 100, X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff45b5f-00b5-4422-aa30-1e0ceac7fe21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
